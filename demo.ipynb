{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model loaded succeed\n"
     ]
    }
   ],
   "source": [
    "from configs import TestConfig\n",
    "import torch\n",
    "import jieba\n",
    "import thulac\n",
    "from utils import text_process, seg_char, BEGIN, SINGLE\n",
    "thu = thulac.thulac(seg_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model loaded succeed\n"
     ]
    }
   ],
   "source": [
    "configs = TestConfig()\n",
    "model = configs.model\n",
    "tokenizer = configs.tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_seg(model, tokenizer, sentence):\n",
    "    \"\"\"Bert 中文分词\n",
    "\n",
    "    Args:\n",
    "        sentence (str): 单个句子.\n",
    "    Returns:\n",
    "        word_list (list [str]): 分词结果.\n",
    "    \"\"\"\n",
    "    sentence = text_process(sentence, is_tradition=True)\n",
    "    char_list = seg_char(sentence)\n",
    "    inputs = tokenizer(\n",
    "        char_list,\n",
    "        return_tensors=\"pt\",\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "    output = model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
    "    # [batch_size, src_len, 4]\n",
    "    output = torch.argmax(output, dim=-1)[:, 1: -1].tolist()[0]\n",
    "    char_list = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])[1: -1]\n",
    "    idxes = [idx for idx, l in enumerate(output) if l == BEGIN or l == SINGLE] + [None]\n",
    "    # print(idxes)\n",
    "    word_list = [\"\".join(char_list[idxes[i]: idxes[i+1]]) for i in range(len(idxes)-1)]\n",
    "    return word_list, char_list, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "be##rt 可以 用 于 中文 分词\nBert 可以 用于 中文 分词\nBert 可以 用于 中文 分词\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Bert可以用于中文分词\"\n",
    "word_list, char_list, output = bert_seg(model, tokenizer, sentence)\n",
    "print(\" \".join(word_list))\n",
    "print(\" \".join(jieba.cut(sentence)))\n",
    "print(\" \".join([x[0] for x in thu.cut(sentence)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "中国青年报 发表 文章 《 做题家 们 的 怒气 为何 要 往 丁真 身上 撒 》\n中国青年报 发表文章 《 做题 家们 的 怒气 为何 要往 丁真 身上 撒 》\n中国 青年报 发表 文章 《 做 题 家 们 的 怒气 为何 要 往 丁真 身上 撒 》\n"
     ]
    }
   ],
   "source": [
    "sentence = \"中国青年报发表文章《做题家们的怒气为何要往丁真身上撒》\"\n",
    "word_list, char_list, output = bert_seg(model, tokenizer, sentence)\n",
    "print(\" \".join(word_list))\n",
    "print(\" \".join(jieba.cut(sentence)))\n",
    "print(\" \".join([x[0] for x in thu.cut(sentence)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "内卷 / 、 / 打工人 / 、 / 做 / 题家 / 、 / 小丑 / 竟是 / 我 / 自己 / 都 / 是 / 近年 / 来 / 较 / 火 / 的 / 自嘲梗\n内 / 卷 / 、 / 打 / 工人 / 、 / 做题 / 家 / 、 / 小丑 / 竟是 / 我 / 自己 / 都 / 是 / 近年来 / 较火 / 的 / 自嘲 / 梗\n内卷 / 、 / 打工 / 人 / 、 / 做 / 题 / 家 / 、 / 小丑 / 竟是 / 我 / 自己 / 都 / 是 / 近年 / 来 / 较 / 火 / 的 / 自嘲梗\n"
     ]
    }
   ],
   "source": [
    "sentence = \"内卷、打工人、做题家、小丑竟是我自己都是近年来较火的自嘲梗\"\n",
    "word_list, char_list, output = bert_seg(model, tokenizer, sentence)\n",
    "print(\" / \".join(word_list))\n",
    "print(\" / \".join(jieba.cut(sentence)))\n",
    "print(\" / \".join([x[0] for x in thu.cut(sentence)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}